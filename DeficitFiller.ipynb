{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b2162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and filtering metadata...\n",
      "‚úÖ Found 825 valid noise files.\n",
      "\n",
      "üìâ Target to fill: 385 files\n",
      "üìÇ Copying from: D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/Audio Wise V1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/385 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Dataset File'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Dataset File'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 62\u001b[0m\n",
      "\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Get info\u001b[39;00m\n",
      "\u001b[1;32m---> 62\u001b[0m original_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataset File\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;32m     63\u001b[0m noise_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass Name\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;32m     65\u001b[0m source_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(NOISE_SOURCE_FOLDER, original_filename)\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:1133\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n",
      "\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n",
      "\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n",
      "\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n",
      "\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:1249\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n",
      "\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n",
      "\u001b[0;32m   1248\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n",
      "\u001b[1;32m-> 1249\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n",
      "\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n",
      "\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n",
      "\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n",
      "\u001b[0;32m   3817\u001b[0m     ):\n",
      "\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n",
      "\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n",
      "\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Dataset File'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===============================\n",
    "# USER SETTINGS\n",
    "# ===============================\n",
    "# 1. Folder containing the FSC22 Audio files (Source)\n",
    "NOISE_SOURCE_FOLDER = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/Audio Wise V1.0\"\n",
    "\n",
    "# 2. Path to the FSC22 Metadata CSV\n",
    "METADATA_PATH = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/Metadata V1.0 FSC22.csv\"\n",
    "\n",
    "# 3. Your EXISTING output folder (Destination)\n",
    "EXISTING_OUTPUT_FOLDER = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/NoEagleSound\"\n",
    "\n",
    "# 4. How many files do you need?\n",
    "TARGET_DEFICIT = 385 \n",
    "\n",
    "# 5. The Allowed Classes\n",
    "ALLOWED_CLASSES = [\n",
    "    'Rain', 'Thunderstorm', 'WaterDrops', 'Wind', 'Silence', \n",
    "    'TreeFalling', 'Whistling', 'Insect', 'Frog', \n",
    "    'BirdChirping', 'WingFlapping', 'Squirrel'\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# STEP 1: LOAD & FILTER METADATA\n",
    "# ===============================\n",
    "print(\"Loading and filtering metadata...\")\n",
    "try:\n",
    "    meta_df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    # Filter by class\n",
    "    filtered_df = meta_df[meta_df['Class Name'].isin(ALLOWED_CLASSES)].copy()\n",
    "    \n",
    "    # Shuffle to get a random mix\n",
    "    filtered_df = filtered_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(filtered_df)} valid noise files.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading metadata: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ===============================\n",
    "# STEP 2: COPY FILES\n",
    "# ===============================\n",
    "created_count = 0\n",
    "segments_data = []\n",
    "\n",
    "print(f\"\\nüìâ Target to fill: {TARGET_DEFICIT} files\")\n",
    "print(f\"üìÇ Copying from: {NOISE_SOURCE_FOLDER}\")\n",
    "\n",
    "# Iterate through the filtered list\n",
    "for index, row in tqdm(filtered_df.iterrows(), total=min(len(filtered_df), TARGET_DEFICIT)):\n",
    "    if created_count >= TARGET_DEFICIT:\n",
    "        break\n",
    "        \n",
    "    # [FIXED]: Updated column name to 'Dataset File Name'\n",
    "    original_filename = str(row['Dataset File Name']).strip()\n",
    "    noise_class = str(row['Class Name']).strip()\n",
    "    \n",
    "    source_path = os.path.join(NOISE_SOURCE_FOLDER, original_filename)\n",
    "    \n",
    "    if not os.path.exists(source_path):\n",
    "        continue\n",
    "        \n",
    "    # Prepare Destination\n",
    "    new_filename = f\"GeneralForest_{noise_class}_{original_filename}\"\n",
    "    \n",
    "    location = \"GeneralForest\"\n",
    "    label = \"NoEagleSound\"\n",
    "    \n",
    "    # Save Folder: Output / GeneralForest / NoEagleSound\n",
    "    save_folder = os.path.join(EXISTING_OUTPUT_FOLDER, location, label)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    dest_path = os.path.join(save_folder, new_filename)\n",
    "    \n",
    "    try:\n",
    "        # COPY THE FILE\n",
    "        shutil.copy2(source_path, dest_path)\n",
    "        \n",
    "        # Add to manifest data\n",
    "        segments_data.append({\n",
    "            'source_audio': original_filename,\n",
    "            'segment_filename': new_filename,\n",
    "            'label': label,\n",
    "            'label_category': label,\n",
    "            'quality': 'N/A', \n",
    "            'output_folder': os.path.join(location, label),\n",
    "            'segment_start_time': 0.0,\n",
    "            'segment_end_time': 5.0,\n",
    "            'segment_duration': 5.0,\n",
    "            'trigger_annotation_start': 'N/A',\n",
    "            'trigger_annotation_end': 'N/A',\n",
    "            'num_annotations': 0,\n",
    "            'selection_numbers': 'None',\n",
    "            'annotation_types': 'None',\n",
    "            'annotation_times': 'None',\n",
    "            'individual_call_details': f'External_{noise_class}',\n",
    "            'group_id': 9999 + created_count,\n",
    "            'overlap_group_id': 9999 + created_count,\n",
    "            'location_id': location\n",
    "        })\n",
    "        \n",
    "        created_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error copying {original_filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚ú® Finished! Copied {created_count} noise files.\")\n",
    "\n",
    "# ===============================\n",
    "# STEP 3: UPDATE MASTER MANIFEST\n",
    "# ===============================\n",
    "manifest_path = os.path.join(EXISTING_OUTPUT_FOLDER, \"master_manifest.csv\")\n",
    "\n",
    "if segments_data:\n",
    "    print(\"Updating master_manifest.csv...\")\n",
    "    try:\n",
    "        # Load existing manifest\n",
    "        if os.path.exists(manifest_path):\n",
    "            existing_df = pd.read_csv(manifest_path)\n",
    "            new_df = pd.DataFrame(segments_data)\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        else:\n",
    "            combined_df = pd.DataFrame(segments_data)\n",
    "            \n",
    "        combined_df.to_csv(manifest_path, index=False)\n",
    "        print(\"‚úÖ Master manifest updated successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating manifest: {e}\")\n",
    "        # Backup save\n",
    "        pd.DataFrame(segments_data).to_csv(os.path.join(EXISTING_OUTPUT_FOLDER, \"added_noise_manifest.csv\"), index=False)\n",
    "        print(\"   Saved new data to 'added_noise_manifest.csv' instead.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
