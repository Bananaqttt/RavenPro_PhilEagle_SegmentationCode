{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4e5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data:   0%|          | 0/11032 [00:00<?, ?files/s]D:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:121: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Miguel\\.cache\\huggingface\\hub\\datasets--Saads--xecanto_birds. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading data: 100%|██████████| 11032/11032 [20:39<00:00,  8.90files/s]\n",
      "Generating train split:   0%|          | 0/11031 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\builder.py:1588\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n",
      "\u001b[0;32m   1579\u001b[0m     writer \u001b[38;5;241m=\u001b[39m writer_class(\n",
      "\u001b[0;32m   1580\u001b[0m         features\u001b[38;5;241m=\u001b[39mwriter\u001b[38;5;241m.\u001b[39m_features,\n",
      "\u001b[0;32m   1581\u001b[0m         path\u001b[38;5;241m=\u001b[39mfpath\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSSSS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJJJJJ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   1586\u001b[0m         embed_local_files\u001b[38;5;241m=\u001b[39membed_local_files,\n",
      "\u001b[0;32m   1587\u001b[0m     )\n",
      "\u001b[1;32m-> 1588\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m record\n",
      "\u001b[0;32m   1589\u001b[0m writer\u001b[38;5;241m.\u001b[39mwrite(example, key)\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\features\\features.py:2054\u001b[0m, in \u001b[0;36mFeatures.encode_example\u001b[1;34m(self, example)\u001b[0m\n",
      "\u001b[0;32m   2053\u001b[0m example \u001b[38;5;241m=\u001b[39m cast_to_python_objects(example)\n",
      "\u001b[1;32m-> 2054\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\features\\features.py:1345\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[1;34m(schema, obj, level)\u001b[0m\n",
      "\u001b[0;32m   1343\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot None but expected a dictionary instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m   1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n",
      "\u001b[1;32m-> 1345\u001b[0m         {k: \u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m schema}\n",
      "\u001b[0;32m   1346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1347\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1348\u001b[0m     )\n",
      "\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (LargeList, List)):\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\features\\features.py:1368\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[1;34m(schema, obj, level)\u001b[0m\n",
      "\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencode_example\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1369\u001b[0m \u001b[38;5;66;03m# Other object should be directly convertible to a native Arrow type (like Translation and Translation)\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\features\\audio.py:108\u001b[0m, in \u001b[0;36mAudio.encode_example\u001b[1;34m(self, value)\u001b[0m\n",
      "\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchcodec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioEncoder  \u001b[38;5;66;03m# needed to write audio files\u001b[39;00m\n",
      "\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\__init__.py:10\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_frame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioSamples, Frame, FrameBatch  \u001b[38;5;66;03m# usort:skip # noqa\u001b[39;00m\n",
      "\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decoders, samplers  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Note that version.py is generated during install.\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\decoders\\__init__.py:7\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n",
      "\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n",
      "\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioStreamMetadata, VideoStreamMetadata\n",
      "\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_audio_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\_core\\__init__.py:8\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n",
      "\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n",
      "\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n",
      "\u001b[0;32m      9\u001b[0m     AudioStreamMetadata,\n",
      "\u001b[0;32m     10\u001b[0m     ContainerMetadata,\n",
      "\u001b[0;32m     11\u001b[0m     get_container_metadata,\n",
      "\u001b[0;32m     12\u001b[0m     get_container_metadata_from_header,\n",
      "\u001b[0;32m     13\u001b[0m     VideoStreamMetadata,\n",
      "\u001b[0;32m     14\u001b[0m )\n",
      "\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n",
      "\u001b[0;32m     16\u001b[0m     _add_video_stream,\n",
      "\u001b[0;32m     17\u001b[0m     _get_backend_details,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     43\u001b[0m     seek_to_pts,\n",
      "\u001b[0;32m     44\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\_core\\_metadata.py:16\u001b[0m\n",
      "\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchcodec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n",
      "\u001b[0;32m     17\u001b[0m     _get_container_json_metadata,\n",
      "\u001b[0;32m     18\u001b[0m     _get_stream_json_metadata,\n",
      "\u001b[0;32m     19\u001b[0m     create_from_file,\n",
      "\u001b[0;32m     20\u001b[0m )\n",
      "\u001b[0;32m     23\u001b[0m SPACES \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\_core\\ops.py:84\u001b[0m\n",
      "\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCould not load libtorchcodec. Likely causes:\u001b[39m\n",
      "\u001b[0;32m     71\u001b[0m \u001b[38;5;124m          1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m     81\u001b[0m     )\n",
      "\u001b[1;32m---> 84\u001b[0m \u001b[43mload_torchcodec_shared_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Note: We use disallow_in_graph because PyTorch does constant propagation of\u001b[39;00m\n",
      "\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# factory functions.\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\_core\\ops.py:69\u001b[0m, in \u001b[0;36mload_torchcodec_shared_libraries\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     64\u001b[0m traceback \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m     66\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, e \u001b[38;5;129;01min\u001b[39;00m exceptions)\n",
      "\u001b[0;32m     67\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[end of libtorchcodec loading traceback].\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m     68\u001b[0m )\n",
      "\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[0;32m     70\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCould not load libtorchcodec. Likely causes:\u001b[39m\n",
      "\u001b[0;32m     71\u001b[0m \u001b[38;5;124m      1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n",
      "\u001b[0;32m     72\u001b[0m \u001b[38;5;124m         versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\u001b[39m\n",
      "\u001b[0;32m     73\u001b[0m \u001b[38;5;124m      2. The PyTorch version (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is not compatible with\u001b[39m\n",
      "\u001b[0;32m     74\u001b[0m \u001b[38;5;124m         this version of TorchCodec. Refer to the version compatibility\u001b[39m\n",
      "\u001b[0;32m     75\u001b[0m \u001b[38;5;124m         table:\u001b[39m\n",
      "\u001b[0;32m     76\u001b[0m \u001b[38;5;124m         https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\u001b[39m\n",
      "\u001b[0;32m     77\u001b[0m \u001b[38;5;124m      3. Another runtime dependency; see exceptions below.\u001b[39m\n",
      "\u001b[0;32m     78\u001b[0m \u001b[38;5;124m    The following exceptions were raised as we tried to load libtorchcodec:\u001b[39m\n",
      "\u001b[0;32m     79\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;32m     80\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m     81\u001b[0m )\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not load libtorchcodec. Likely causes:\n",
      "          1. FFmpeg is not properly installed in your environment. We support\n",
      "             versions 4, 5, 6, and 7 on all platforms, and 8 on Mac and Linux.\n",
      "          2. The PyTorch version (2.9.1+cpu) is not compatible with\n",
      "             this version of TorchCodec. Refer to the version compatibility\n",
      "             table:\n",
      "             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n",
      "          3. Another runtime dependency; see exceptions below.\n",
      "        The following exceptions were raised as we tried to load libtorchcodec:\n",
      "        \n",
      "[start of libtorchcodec loading traceback]\n",
      "FFmpeg version 8: Could not load this library: D:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll\n",
      "FFmpeg version 7: Could not load this library: D:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\n",
      "FFmpeg version 6: Could not load this library: D:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\n",
      "FFmpeg version 5: Could not load this library: D:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\n",
      "FFmpeg version 4: Could not load this library: D:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n",
      "[end of libtorchcodec loading traceback].\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n",
      "\u001b[0;32m     18\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(audio_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n",
      "\u001b[1;32m---> 21\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     23\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataset), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset)):\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\load.py:1417\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n",
      "\u001b[0;32m   1414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n",
      "\u001b[0;32m   1416\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n",
      "\u001b[1;32m-> 1417\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1423\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1425\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n",
      "\u001b[0;32m   1426\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   1427\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n",
      "\u001b[0;32m   1428\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\builder.py:897\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n",
      "\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m    896\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n",
      "\u001b[1;32m--> 897\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n",
      "\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\builder.py:1612\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n",
      "\u001b[0;32m   1611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n",
      "\u001b[1;32m-> 1612\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n",
      "\u001b[0;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\builder.py:973\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n",
      "\u001b[0;32m    969\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n",
      "\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    972\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n",
      "\u001b[1;32m--> 973\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n",
      "\u001b[0;32m    976\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    977\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m    978\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    979\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n",
      "\u001b[0;32m    980\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\builder.py:1450\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n",
      "\u001b[0;32m   1448\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;32m   1449\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n",
      "\u001b[1;32m-> 1450\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_prepare_split_args\u001b[49m\n",
      "\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[0;32m   1453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[0;32m   1454\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\n",
      "File \u001b[1;32mD:\\_3rd Year Class\\1st Sem\\Machine Learning\\_ForLE\\For DataSet\\Code\\.venv\\Lib\\site-packages\\datasets\\builder.py:1607\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n",
      "\u001b[0;32m   1605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, SchemaInferenceError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m   1606\u001b[0m         e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39m__context__\n",
      "\u001b[1;32m-> 1607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;32m   1609\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_NAME = \"Saads/xecanto_birds\"\n",
    "OUTPUT_DIR = r\"D:\\MyDatasets\\xecanto_birds\"\n",
    "\n",
    "audio_dir = os.path.join(OUTPUT_DIR, \"audio\")\n",
    "os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "# Load dataset WITHOUT decoding audio\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\", streaming=True)\n",
    "\n",
    "rows = []\n",
    "\n",
    "HF_BASE_URL = \"https://huggingface.co/datasets/Saads/xecanto_birds/resolve/main/\"\n",
    "\n",
    "for i, item in tqdm(enumerate(dataset)):\n",
    "    audio_info = item[\"audio\"]\n",
    "\n",
    "    # The dataset gives \"path\" like \"audio/xxx.wav\"\n",
    "    relative_path = audio_info[\"path\"]\n",
    "\n",
    "    # Build real download URL\n",
    "    download_url = HF_BASE_URL + relative_path\n",
    "\n",
    "    # Determine extension\n",
    "    ext = relative_path.split(\".\")[-1]\n",
    "\n",
    "    # Local file path\n",
    "    file_path = os.path.join(audio_dir, f\"{i}.{ext}\")\n",
    "\n",
    "    # Download audio\n",
    "    resp = requests.get(download_url)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "    # Save metadata\n",
    "    meta = item.copy()\n",
    "    meta[\"file_path\"] = file_path\n",
    "    rows.append(meta)\n",
    "\n",
    "# Export CSV\n",
    "csv_path = os.path.join(OUTPUT_DIR, \"metadata.csv\")\n",
    "pd.DataFrame(rows).to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"DONE! Audio + CSV saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
